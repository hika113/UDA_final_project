{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured Data Analysis for Policy (94-775)\n",
    "### Final Project Report\n",
    "### Hikaru Murase (hikarum), Nidhi Shree (nshree)\n",
    "### May 3, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (1) Collecting Data of Image Files\n",
    "* Note: If you download the data from Google Drive, you can skip this part (move to part (2)).\n",
    "\n",
    "### (a) Bing Image Search (damaged and undamaged buildings)\n",
    "First, we collected image files from Bing image search results using Web API (Bing Image Search API v7). The reason why we chose Bing instead of Google is that Google does not give us over 100 image files per keyword. The issue seems to remain unsolved. Although Bing provided us with more than 100 image files, the total number still did not reach 1,000. (Even we searched more popular words such as \"cat\" in this API, we could not obtain over 1,000 files.)  \n",
    "  \n",
    "In conclusion, we obatained approximately 300 pictures of damaged buildings and 500 pictures of undamaged buildings from Bing image search. In these files, most of the images appear to contain single building, while some images do not appear to contain buildings.  \n",
    "  \n",
    "Also, as a data augmentation process, we flipped the images horizontally and add the flipped images to the data. Thus, the volume of data doubled (damaged: 600, undamaged: 1,000 in total)  \n",
    "  \n",
    "* Note 1: Before running the code below, plase create a new folder (named \"images\") in the folder where this ipynb file exists. Then, create two new folders (named \"damaged\" and \"undamaged\", respectively) directly under \"images\" folder. Thus, the structure of the folders should be as follows: >base folder > \"images\" > \"damaged\" and \"undamaged\"\n",
    "* Note 2: Downloaded image files should be stored in \"damaged\" folder for images of damaged buildings and \"undamaged\" folder for images of undamaged buildings. You need to transfer files downloaded by the code below to each folder (\"damaged\" or \"undamaged\") manually at the end of (a) and (b).\n",
    "* Note 3: You need to run all the code in part (a) twice to obtain images of \"undamaged\" buildings and \"damaged\" buildings separately. The details are describred in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is a preparation for using web API in Bing image search.\n",
    "subscription_key = '859122352e0342e299ac923d58604c24' # This is my API key \n",
    "   # (The number of calls is limited to 1,000 per month.)\n",
    "\n",
    "assert subscription_key\n",
    "search_url = \"https://api.cognitive.microsoft.com/bing/v7.0/images/search\"\n",
    "\n",
    "# You need to run all the code in part (a) twice to get two kinds of data. \n",
    "# (1) In the first time, assign search term as \"single building.\"\n",
    "# (2) In the second time (after you finish data augmentation for images of 'single building'),\n",
    "# assign search term as \"single collapsed building\" \n",
    "\n",
    "search_term = \"single building\" # (1) to search images of undamaged buildings\n",
    "#search_term = \"Single collapsed building\" # (2) to search images of damaged buildings\n",
    "\n",
    "count = 150 # number of images to get per attempt (max: 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total estimate matches (the number of images hit by the search term):  609\n",
      "The next offset is 187 .\n",
      "The next offset is 371 .\n",
      "The next offset is 565 .\n",
      "The next offset is 642 (this is the end point).\n",
      "The total number of images obtained: 491\n"
     ]
    }
   ],
   "source": [
    "# This code will extract image files' thumbnail urls and encoding formats \n",
    "import requests\n",
    "\n",
    "# 1st attempt of a loop process\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "params  = {\"count\": count, \"q\": search_term, \"offset\": 0, \"license\": \"any\", \"imageType\": \"photo\"}  \n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "search_results = response.json() \n",
    "\n",
    "thumbnail_urls = []\n",
    "content_urls = []\n",
    "formats = []\n",
    "\n",
    "for img in search_results[\"value\"]:\n",
    "    thumbnail_urls.append(img[\"thumbnailUrl\"]) # extract and store the thumbnail URLS.\n",
    "    content_urls.append(img[\"contentUrl\"]) # extract and store the content URLS\n",
    "    formats.append(img[\"encodingFormat\"]) # extract and store the encoding formats\n",
    "\n",
    "# the following code extracts total estimate matches.\n",
    "total_num_files = search_results[\"totalEstimatedMatches\"]\n",
    "\n",
    "# the following code extracts the next offset (\"The zero-based offset that indicates \n",
    "# the number of images to skip before returning images. The default is 0. \n",
    "# The offset should be less than (totalEstimatedMatches - count)\")\n",
    "offset = search_results[\"nextOffset\"]\n",
    "\n",
    "print(\"Total estimate matches (the number of images hit by the search term): \", total_num_files)\n",
    "print(\"The next offset is\", offset, \".\")\n",
    "\n",
    "\n",
    "while offset < total_num_files - count: \n",
    "    # Extract image files (2nd, 3rd, ..., and the last-1 attempt of the loop)\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "    params  = {\"count\": count, \"q\": search_term, \"offset\": offset, \"license\": \"any\", \"imageType\": \"photo\"}  \n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    for img in search_results[\"value\"]:\n",
    "        thumbnail_urls.append(img[\"thumbnailUrl\"])\n",
    "        content_urls.append(img[\"contentUrl\"])\n",
    "        formats.append(img[\"encodingFormat\"])\n",
    "    offset = search_results[\"nextOffset\"]\n",
    "    print(\"The next offset is\", offset, \".\")\n",
    "else: \n",
    "    count = total_num_files - offset - 1\n",
    "    if count > 0: # Extract the remainig image files (the last attempt of the loop)\n",
    "        headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "        params  = {\"count\": count, \"q\": search_term, \"offset\": offset, \"license\": \"any\", \"imageType\": \"photo\"}  \n",
    "        response = requests.get(search_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        search_results = response.json()\n",
    "        for img in search_results[\"value\"]:\n",
    "            thumbnail_urls.append(img[\"thumbnailUrl\"])\n",
    "            content_urls.append(img[\"contentUrl\"])\n",
    "            formats.append(img[\"encodingFormat\"])\n",
    "        offset = search_results[\"nextOffset\"]\n",
    "        print(\"The next offset is\", offset, \"(this is the end point).\")\n",
    "    print(\"The total number of images obtained:\", len(thumbnail_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will download the images to the same folder as this ipynb file (this will take some minutes)\n",
    "for i, url in enumerate(thumbnail_urls): # if you want to download thumbnail files choose this line\n",
    "#for i, url in enumerate(content_urls): # if you want to download content (full size) files choose this line\n",
    "    fileName = str(i+1) + \".\" +formats[i]\n",
    "    req = requests.get(url)\n",
    "    f = open(fileName, 'wb')\n",
    "    f.write(req.content)\n",
    "    f.close()\n",
    "    \n",
    "##### NOTE: #####\n",
    "# To apply data augmentation below, you still need to keep downloaded files in the same folder as the folder containing your ipynb file.\n",
    "# (Do not transfer downloaded files until you complete the data augmentation by the below code.)\n",
    "# After data augmentation below code, you need to (create \"undamaged\" or \"damaged\" folder and) transfer these files into the folder manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data augmentation, we flipped the images horizontally and add the flipped images to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will doncuct data augmentation (flip horizontally) for the downloaded files by above code.\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "for i, url in enumerate(thumbnail_urls): # if you downloaded thumbnail files choose this line\n",
    "#for i, url in enumerate(content_urls): # if you downloaded content (full size) files choose this line\n",
    "    fileName = str(i+1) + \".\" +formats[i]\n",
    "    fileName_mir = \"mir_\" + str(i+1) + \".\" +formats[i]\n",
    "    img = Image.open(fileName)\n",
    "    img_mir = ImageOps.mirror(img)\n",
    "    if formats[i] == \"jpg\" or formats[i] == \"jpeg\" or formats[i] == \"gif\" or formats[i] == \"png\":\n",
    "        # This is for the purpose of avoiding error due to irregular extention such as \".animatedgif\"\n",
    "        img_mir.save(fileName_mir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note 1: After applying the code above, you will need to transfer these files into the specified folder (\"damaged\" or \"undamaged\") manually. \n",
    "* Note 2: Once you transfer the files, you can re-run the code to obtain images of damaged buildings by changing the search query. If you have run the code in part (a) twice (i.e. you have obtained images of both undamaged and damaged from Bing Image Search), you can move to part (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Natural Hazards Image Database (damaged buildings)\n",
    "Second, we obtained images of damaged buildings from Natural Hazards Image Database where there are approximately 100 photos taken in the stricken areas by earthquakes such as the Great East Japan Earthquake in 2011. We used web scraping to obtain the data. In summary, we gained 83 images. As in part (a), we conducted data augmentation (flipped horizontally) to double the data. Therefore, we obtained 166 images of damaged buildings in total. While most images contain damaged (collapsed) buildings, some of the pictures contain several (not single) buildings and a few pictures do not appear to contain buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note:Before running the code below, make sure that you transferred image files downloaded in part (a) to the \"undamaged\" or \"damaged\" folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will extract urls of images from Natural Hazards Image Database by web scraping.\n",
    "\n",
    "# You can extract 39 images of 2011 Honshu Japan Earthquake and Tsunami from the following code.\n",
    "r1 = requests.get('https://www.ngdc.noaa.gov/hazardimages/event/show/256') \n",
    "\n",
    "# Extract 24 pictures of 2011 Christchurch New Zealand Earthquake\n",
    "r2 = requests.get('https://www.ngdc.noaa.gov/hazardimages/event/show/259') \n",
    "\n",
    "# Extract 20 pictures of 2010 Haiti Earthquake and Tsunami\n",
    "r3 = requests.get('https://www.ngdc.noaa.gov/hazardimages/event/show/258') \n",
    "\n",
    "links = []\n",
    "for i in [1,2,3]:\n",
    "    page = eval(\"r\"+str(i)).content\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    lists = soup.find_all(\"li\", class_ = \"\")\n",
    "    for list_ in lists:\n",
    "        link = list_.img.get(\"src\")\n",
    "        links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will download the pictures urls of which are stored in \"lists\"\n",
    "for link in links:\n",
    "    fileName = link.split(\"/\")[-1]\n",
    "    req = requests.get(link)\n",
    "    f = open(fileName, 'wb')\n",
    "    f.write(req.content)\n",
    "    f.close()\n",
    "\n",
    "##### NOTE #####\n",
    "# To apply data augmentation below, you still need to keep downloaded files in the same folder as the folder containing your ipynb file.\n",
    "# (Do not transfer downloaded files until you complete the data augmentation by the below code.)\n",
    "# After data augmentation below code, you need to transfer these files into the \"damaged\" folder manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will conduct data augmentation (flip horizontally) \n",
    "for link in links:\n",
    "    fileName = link.split(\"/\")[-1]\n",
    "    fileName_mir = \"mir_\" + link.split(\"/\")[-1]\n",
    "    img = Image.open(fileName)\n",
    "    img_mir = ImageOps.mirror(img)\n",
    "    img_mir.save(fileName_mir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: After downloading, you will need to transfer these files into the \"damaged\" folder manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Other Sources\n",
    "Finally, we acquired image data of undamaged buildings from The Oxford Building Dataset, which offers approximately 5,000 pictures collected from Flickr by searching for particular Oxford landmarks; however, many of the images are pictures of humans or pictures taken inside buildings. Thus, we decided not use this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference List\n",
    "- Microsoft Azure, \"Image Search API v7 reference\", https://docs.microsoft.com/en-us/rest/api/cognitiveservices/bing-images-api-v7-reference\n",
    "- \"Do you know why only 100 images is the limit? Can we have more than 100?\", https://github.com/hardikvasa/google-images-download/issues/7\n",
    "- \"ImageOps Module\" http://pillow.readthedocs.io/en/latest/reference/ImageOps.html#PIL.ImageOps.mirror\n",
    "- \"Natural Hazards Image Database\", https://www.ngdc.noaa.gov/hazardimages/\n",
    "- \"Do you know why only 100 images is the limit? Can we have more than 100?\", https://github.com/hardikvasa/google-images-download/issues/7\n",
    "- James Philbin, Relja ArandjeloviÄ‡ and Andrew Zisserman, \"The Oxford Buildings Dataset\", http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (2) Image Analysis (Establishing Models)\n",
    "Using the collected images (763 image files of damaged buildings and 1,160 image files of undamaged buildings), we conducted image analysis to establish classifiers which can classify \"damaged\" and \"undamaged\" buildings.  \n",
    "  \n",
    "First, we resized images and flattened them to make it easier to analyze. We labeled each image as \"damaged\" (y = 1) or \"undamaged\" (y = 0).  \n",
    "  \n",
    "Second, we divided the images into a training set and a test set randomly.  \n",
    "  \n",
    "Third, we conducted Principal Component Analysis (PCA) to reduce dimensions of image features. As the code below demonstrates, the first six principal components explain the variance well. Thus, we chose the number of components as six.  \n",
    "  \n",
    "Fourth, since this is a classification problem, we used Support Vector Machine (SVM) classifier, k-Nearest Neighbor (k-NN), Random Forest (RM) classifier, and neural network (NN) as classifiers. For SVM, k-NN, and RM, we performed 10-fold cross-validation to choose the best hyperparameter (C for SVM classifier, k for k-NN, and the number of trees (b) for RM classifier). Regarding neural network, we tried a single layer neural net, a two-layer neural net, and a two-layer neural net without dimension reduction.    \n",
    "  \n",
    "Finally, we evaluated the models with the best hyperparameters by computing a test error rate or accuracy and created a confusion matrix for each model for part (3).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import glob\n",
    "# You need to install \"keras\" and \"tensorflow\" packages in your Python to run this code.\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will create functions to transform images\n",
    "\n",
    "# Define the size after resizing\n",
    "standard_size = (300, 167)\n",
    "\n",
    "# Resize an image to the standard_size (shape = (167, 300, 3))\n",
    "# and convert it numpy array of RGB pixels\n",
    "def img_to_matrix(filename):\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize(standard_size)\n",
    "    img_array = np.array(img)\n",
    "    return img_array\n",
    "\n",
    "# Flatten a (m, n, l) numpy array into an array of shape (1, m * n * l)\n",
    "def flatten_image(img):\n",
    "    if len(img.shape)==3: # To avoid an error due to a few narray with an exceptional shape (m, n)\n",
    "            s = img.shape[0] * img.shape[1] * img.shape[2]\n",
    "            img_wide = img.reshape(1, s)\n",
    "            return img_wide[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will list each image files within \"damaged\" and \"undamaged\" folders in \"image\" folder \n",
    "# and label \"damaged\" to the files in \"damaged\" folder and \"undamaged\" to the files in \"undamaged\" folder \n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for image in glob.glob(\"./images/damaged/*.jpg\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"damaged\")\n",
    "for image in glob.glob(\"./images/damaged/*.jpeg\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"damaged\")\n",
    "for image in glob.glob(\"./images/damaged/*.gif\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"damaged\")\n",
    "for image in glob.glob(\"./images/damaged/*.png\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"damaged\")\n",
    "      \n",
    "for image in glob.glob(\"./images/undamaged/*.jpg\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"undamaged\")\n",
    "for image in glob.glob(\"./images/undamaged/*.jpeg\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"undamaged\")\n",
    "for image in glob.glob(\"./images/undamaged/*.gif\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"undamaged\")\n",
    "for image in glob.glob(\"./images/undamaged/*.png\"):\n",
    "    images.append(image)\n",
    "    labels.append(\"undamaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will create a function to process the data (convert image files into a numpy array)\n",
    "def data_processing(images, labels):         \n",
    "    data = []\n",
    "    \n",
    "    del_row = []\n",
    "    for i, image in enumerate(images):\n",
    "        # Apply the created functions above\n",
    "        img = img_to_matrix(image)\n",
    "        img = flatten_image(img)\n",
    "        \n",
    "        if img is not None: # To remove null row (the image whose shape was not (m, n, l) numpy array)\n",
    "            data.append(img)\n",
    "        if img is None:\n",
    "            del_row.append(i)\n",
    "\n",
    "    # Convert \"data\" list into numpy array\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Remove the corresponding rows from \"labels\"\n",
    "    labels = np.delete(labels, del_row, axis = 0)\n",
    "    print(\"Images in row #\", del_row, \"were removed due to the irregular shape.\")\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in row # [722, 723, 724, 725] were removed due to the irregular shape.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19880338, 0.0942502 , 0.05668722, 0.04152947, 0.03914788,\n",
       "       0.02474766, 0.02014306, 0.0161251 , 0.01382254, 0.0136698 ,\n",
       "       0.01188621, 0.01024629, 0.00867681, 0.00847302, 0.00776684,\n",
       "       0.00703954, 0.00673103, 0.00659129, 0.00557721, 0.00556092])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will print the variance ratio explained by each principle component.\n",
    "# We can check the explained variance ratio by PCA to determine the number of principal components.\n",
    "data, cleaned_labels = data_processing(images, labels)\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "pca.explained_variance_ratio_[:20] # display only the first ten (20) principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that the first principal component explains 19.9%, the second explains 9.4%, the third explains 5.7%, the fourth explains 4.2%, the fifth explains 2.5%, and the sixthe explains 2.0% of the variance. Thus, we chose the number of components as six (6).  \n",
    "  \n",
    "Then, we labeled each image as \"damaged\" (y = 1) or \"undamaged\" (y = 0), divided the images into a training set and a test set randomly, and conducted Principal Component Analysis (PCA) to reduce dimensions of image features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in row # [722, 723, 724, 725] were removed due to the irregular shape.\n",
      "The number of images of damaged buildings in total: 756\n",
      "The number of images of undamaged buildings in total: 1154\n",
      "The number of training samples: 1528\n",
      "The number of images of damaged buildings in the training samples: 593\n",
      "The number of images of undamaged buildings in the training samples: 935\n",
      "The number of test samples: 382\n",
      "The number of images of damaged buildings in the test samples: 163\n",
      "The number of images of undamaged buildings in the test samples: 219\n"
     ]
    }
   ],
   "source": [
    "# This code will divide data into a training set and a test set randomly, \n",
    "# then conduct dimensionality reduction with PCA (dimension will reduce from 150300 to 6)\n",
    "\n",
    "data, new_labels = data_processing(images, labels)\n",
    "y = np.where(np.array(new_labels) == 'damaged', 1, 0) # Labeleach image as \"damaged\" (y = 1) and \"undamaged\" (y = 0)\n",
    "np.random.seed(94774)\n",
    "random_ordering_of_data = np.random.permutation(len(data))\n",
    "train_frac = 0.80 # Define the ratio of training data as 80% of the entire data\n",
    "num_training_data = int(train_frac * len(data))\n",
    "train_indices = random_ordering_of_data[:num_training_data]\n",
    "test_indices = random_ordering_of_data[num_training_data:]\n",
    "train_x = data[train_indices]\n",
    "train_y = y[train_indices]\n",
    "test_x = data[test_indices]\n",
    "test_y = y[test_indices]\n",
    "\n",
    "print(\"The number of images of damaged buildings in total:\", len(y[y==1]))\n",
    "print(\"The number of images of undamaged buildings in total:\", len(y[y==0]))\n",
    "      \n",
    "print(\"The number of training samples:\", len(train_y))\n",
    "print(\"The number of images of damaged buildings in the training samples:\", len(train_y[train_y==1]))\n",
    "print(\"The number of images of undamaged buildings in the training samples:\", len(train_y[train_y==0]))\n",
    "\n",
    "print(\"The number of test samples:\", len(test_y))\n",
    "print(\"The number of images of damaged buildings in the test samples:\", len(test_y[test_y==1]))\n",
    "print(\"The number of images of undamaged buildings in the test samples:\", len(test_y[test_y==0]))\n",
    "\n",
    "## Reduce demension to 6 principal components\n",
    "pca = PCA(n_components = 6)\n",
    "train_x = pca.fit_transform(train_x)\n",
    "test_x = pca.fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x174e7e18390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x174de0292e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This code will plot the 1st and 2nd principle components.\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(data)\n",
    "df = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1], \"label\": np.where(y == 1, 'damaged', 'undamaged')})\n",
    "colors = ['red', 'yellow']\n",
    "for label, color in zip(df['label'].unique(), colors):\n",
    "    mask = df['label'] == label\n",
    "    pl.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot above, it appears that there is no large difference of the first and the second components between damaged buildings and undamaged buildings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Support Vector Machine (SVM) classifier  \n",
    "First, we conducted 10-fold cross-validation in the training dataset to choose the best C for the SVM classifier.  \n",
    "Then, we fitted the model with the best hyperparameter to the entire training dataset and evaluated the model in the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will create a function to conduct cross-validation to choose the best hyperparameter.\n",
    "def CV_SVC(train_x, train_y):\n",
    "    lowest_cross_val_error = np.inf\n",
    "    best_C = None\n",
    "    indices = range(len(train_x))\n",
    "    kf = KFold(n_splits=10, random_state=94775)\n",
    "\n",
    "    for C in [1e-2, 1e-1, 1, 10, 100, 1000]:\n",
    "        errors = []\n",
    "        for train_indices, val_indices in kf.split(indices):\n",
    "            classifier = LinearSVC(C=C, random_state=94775)\n",
    "            classifier.fit(train_x[train_indices], train_y[train_indices])\n",
    "            predicted_val_labels = classifier.predict(train_x[val_indices])\n",
    "            error = np.mean(predicted_val_labels != train_y[val_indices])\n",
    "            errors.append(error)\n",
    "        \n",
    "        cross_val_error = np.mean(errors)\n",
    "        print('C:', C, 'cross validation error:', cross_val_error)\n",
    "\n",
    "        if cross_val_error < lowest_cross_val_error:\n",
    "            lowest_cross_val_error = cross_val_error\n",
    "            best_C = C\n",
    "\n",
    "    print('Best C:', best_C, 'cross validation error:', lowest_cross_val_error)\n",
    "    return best_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01 cross validation error: 0.48630460956312344\n",
      "C: 0.1 cross validation error: 0.4856510147918816\n",
      "C: 1 cross validation error: 0.4856510147918816\n",
      "C: 10 cross validation error: 0.4856510147918816\n",
      "C: 100 cross validation error: 0.4856510147918816\n",
      "C: 1000 cross validation error: 0.4856510147918816\n",
      "Best C: 0.1 cross validation error: 0.4856510147918816\n",
      "Test_set_error of SVM classifier with the best C: 0.5052356020942408\n",
      "The confusion matrix is as follows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          105  114\n",
       "1           79   84"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will fit the model with the best hyperparameter to the train set\n",
    "# and evaluate the best SVM classifier in the test set\n",
    "\n",
    "# Apply the function above to assign the best hyperparameter\n",
    "best_C = CV_SVC(train_x, train_y)\n",
    "\n",
    "# Fit the classifier with the best C to the \"entire\" training set\n",
    "final_svm_classifier = LinearSVC(C=best_C, random_state=94775)\n",
    "final_svm_classifier.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate the model in the \"unseen\" test set\n",
    "predicted_test_labels = final_svm_classifier.predict(test_x)\n",
    "test_set_error = np.mean(predicted_test_labels != test_y)\n",
    "print(\"Test_set_error of SVM classifier with the best C:\", test_set_error)\n",
    "\n",
    "# Create a confusion matrix\n",
    "print(\"The confusion matrix is as follows:\")\n",
    "cm_svc = pd.crosstab(test_y, predicted_test_labels, rownames=['Actual'], colnames=['Predicted'])\n",
    "cm_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the the best hyperperameter for a SVM classifier is 0.1 (C = 0.1), and the test set error of the best SVM classifier is 50.5% (accuracy is 49.5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) k-Nearest Neighbor (k-NN) \n",
    "First, we conducted 10-fold cross-validation in the training dataset to choose the best k for the k-Nearest Neighbor.  \n",
    "Then, we fitted the model with the best hyperparameter to the entire training dataset and evaluated the model in the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will create a function to conduct cross-validation to choose the best hyperparameter.\n",
    "def CV_kNN(train_x, train_y):\n",
    "    lowest_cross_val_error = np.inf\n",
    "    best_k = None\n",
    "    indices = range(len(train_x))\n",
    "#    kf = KFold(n_splits=10, shuffle=True, random_state=94775)\n",
    "    kf = KFold(n_splits=10, random_state=94775)\n",
    "    for k in [3, 5, 15, 25, 50]:\n",
    "        errors = []\n",
    "        for train_indices, val_indices in kf.split(indices):\n",
    "            classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "            classifier.fit(train_x[train_indices], train_y[train_indices])\n",
    "            predicted_val_labels = classifier.predict(train_x[val_indices])\n",
    "            error = np.mean(predicted_val_labels != train_y[val_indices])\n",
    "            errors.append(error)\n",
    "        \n",
    "        cross_val_error = np.mean(errors)\n",
    "        print('k:', k, 'cross validation error:', cross_val_error)\n",
    "\n",
    "        if cross_val_error < lowest_cross_val_error:\n",
    "            lowest_cross_val_error = cross_val_error\n",
    "            best_k = k\n",
    "\n",
    "    print('Best k:', best_k, 'cross validation error:', lowest_cross_val_error)\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3 cross validation error: 0.3619754041967664\n",
      "k: 5 cross validation error: 0.33384072927416575\n",
      "k: 15 cross validation error: 0.3652476780185759\n",
      "k: 25 cross validation error: 0.38161764705882356\n",
      "k: 50 cross validation error: 0.38095115239078087\n",
      "Best k: 5 cross validation error: 0.33384072927416575\n",
      "Test_set_error of k-Nearest Neighbor with the best k: 0.306282722513089\n",
      "The confusion matrix is as follows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          180  39\n",
       "1           78  85"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will fit the model with the best hyperparameter to the train set\n",
    "# and evaluate the best SVM classifier in the test set\n",
    "\n",
    "# Apply the function above to assign the best hyperparameter\n",
    "best_k = CV_kNN(train_x, train_y)\n",
    "\n",
    "# Fit the classifier with the best hyperparameter to the \"entire\" training set\n",
    "final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn_classifier.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate the model in the \"unseen\" test set\n",
    "predicted_test_labels = final_knn_classifier.predict(test_x)\n",
    "test_set_error = np.mean(predicted_test_labels != test_y)\n",
    "print(\"Test_set_error of k-Nearest Neighbor with the best k:\", test_set_error)\n",
    "\n",
    "# Create a confusion matrix\n",
    "print(\"The confusion matrix is as follows:\")\n",
    "cm_knn = pd.crosstab(test_y, predicted_test_labels, rownames=['Actual'], colnames=['Predicted'])\n",
    "cm_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the the best hyperperameter for a k-Nearest Neighbor is 5 (k = 5), and the test set error of the best SVM classifier is 30.6% (accuracy is 69.4%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Random Forest classifier (RF) \n",
    "First, we conducted 10-fold cross-validation in the training dataset to choose the best b (number of trees) for the Random Forest classifier.  \n",
    "Then, we fitted the model with the best hyperparameter to the entire training dataset and evaluated the model in the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will create a function to conduct cross-validation to choose the best hyperparameter.\n",
    "def CV_RF(train_x, train_y):\n",
    "    lowest_cross_val_error = np.inf\n",
    "    best_b = None\n",
    "    indices = range(len(train_x))\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=94775)\n",
    "    for b in [50, 100, 150, 200, 250]:\n",
    "        errors = []\n",
    "        for train_indices, val_indices in kf.split(indices):\n",
    "            classifier = RandomForestClassifier(n_estimators=b, random_state=94775)\n",
    "            classifier.fit(train_x[train_indices], train_y[train_indices])\n",
    "            predicted_val_labels = classifier.predict(train_x[val_indices])\n",
    "            error = np.mean(predicted_val_labels != train_y[val_indices])\n",
    "            errors.append(error)\n",
    "        \n",
    "        cross_val_error = np.mean(errors)\n",
    "        print('b:', b, 'cross validation error:', cross_val_error)\n",
    "\n",
    "        if cross_val_error < lowest_cross_val_error:\n",
    "            lowest_cross_val_error = cross_val_error\n",
    "            best_b = b\n",
    "\n",
    "    print('Best b:', best_b, 'cross validation error:', lowest_cross_val_error)\n",
    "    return best_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 50 cross validation error: 0.22705108359133125\n",
      "b: 100 cross validation error: 0.21788785689714482\n",
      "b: 150 cross validation error: 0.21658926728586173\n",
      "b: 200 cross validation error: 0.21526487788097698\n",
      "b: 250 cross validation error: 0.21526917784657726\n",
      "Best b: 200 cross validation error: 0.21526487788097698\n",
      "Test_set_error of Random Forest with the best b: 0.3298429319371728\n",
      "The confusion matrix is as follows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          190  29\n",
       "1           97  66"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will fit the model with the best hyperparameter to the train set\n",
    "# and evaluate the best SVM classifier in the test set\n",
    "\n",
    "# Apply the function above to assign the best hyperparameter\n",
    "best_b = CV_RF(train_x, train_y)\n",
    "\n",
    "# Fit a classifier with the best b to the \"entire\" training set\n",
    "final_rf_classifier = RandomForestClassifier(n_estimators=best_b, random_state=94775)\n",
    "final_rf_classifier.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate the model in the \"unseen\" test set\n",
    "predicted_test_labels = final_rf_classifier.predict(test_x)\n",
    "test_set_error = np.mean(predicted_test_labels != test_y)\n",
    "print(\"Test_set_error of Random Forest with the best b:\", test_set_error)\n",
    "\n",
    "# Create a confusion matrix\n",
    "print(\"The confusion matrix is as follows:\")\n",
    "cm_rf = pd.crosstab(test_y, predicted_test_labels, rownames=['Actual'], colnames=['Predicted'])\n",
    "cm_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the the best hyperperameter for a Random Forest classifier is 200 (b = 200), and the test set error of the best SVM classifier is 33.0% (accuracy is 67.0%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Neural network (NN) \n",
    "First, we .  \n",
    "Then, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Single-layer neural net\n",
    "We..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will...\n",
    "#code to apply neural nets\n",
    "from keras.utils import to_categorical\n",
    "train_labels_categorical = to_categorical(train_y)\n",
    "test_labels_categorical = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -816.26016965,  8079.45356229,  3492.83630898,  -672.21957669,\n",
       "       -4377.6510817 ,  2665.30219031])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_categorical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# This code will...\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "shallow_single_layer_mod = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers\n",
    "shallow_single_layer_mod.add(layers.Dense(2, activation='softmax', input_shape=(6,))) #since it is a 2 class classifier, therefore 2\n",
    "\n",
    "# the line below is not required for constructing the model and just gives a summary of the model\n",
    "print(shallow_single_layer_mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will...\n",
    "\n",
    "# the lecture slides explains the loss (which is an error function) that the optimizer will try to minimize;\n",
    "# there are some optimizers that are popularly used for training neural nets such as stochastic gradient\n",
    "# descent (SGD), RMSProp, and Adam (for the purposes of this class, don't worry about what these are)\n",
    "shallow_single_layer_mod.compile(optimizer='rmsprop',\n",
    "                                   loss='categorical_crossentropy',\n",
    "                                   metrics=['accuracy'])  # metrics says what accuracy metrics to display when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1222 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "1222/1222 [==============================] - 0s 260us/step - loss: 8.9376 - acc: 0.4452 - val_loss: 8.9018 - val_acc: 0.4477\n",
      "Epoch 2/5\n",
      "1222/1222 [==============================] - 0s 24us/step - loss: 8.9608 - acc: 0.4435 - val_loss: 8.9018 - val_acc: 0.4477\n",
      "Epoch 3/5\n",
      "1222/1222 [==============================] - 0s 21us/step - loss: 8.9571 - acc: 0.4435 - val_loss: 8.9354 - val_acc: 0.4444\n",
      "Epoch 4/5\n",
      "1222/1222 [==============================] - 0s 28us/step - loss: 8.9398 - acc: 0.4452 - val_loss: 9.0072 - val_acc: 0.4412\n",
      "Epoch 5/5\n",
      "1222/1222 [==============================] - 0s 30us/step - loss: 8.9624 - acc: 0.4435 - val_loss: 8.9623 - val_acc: 0.4412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174f61c54a8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will...\n",
    "\n",
    "shallow_single_layer_mod.fit(train_x,\n",
    "                               train_labels_categorical,\n",
    "                               validation_split=0.2,\n",
    "                               epochs=5,\n",
    "                               batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 38us/step\n",
      "Test accuracy: 0.41099476658236916\n"
     ]
    }
   ],
   "source": [
    "# This code will...\n",
    "test_loss, test_acc = shallow_single_layer_mod.evaluate(test_x,\n",
    "                                                          test_labels_categorical)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) 2-layer neural net\n",
    "We..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 250)               1750      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 2,252\n",
      "Trainable params: 2,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# This code will...\n",
    "two_layer_model = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers\n",
    "two_layer_model.add(layers.Dense(250, activation='relu', input_shape=(6,))) #optimized to get best accuracy at 250\n",
    "two_layer_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "print(two_layer_model.summary())\n",
    "\n",
    "\n",
    "two_layer_model.compile(optimizer='rmsprop',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1222 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "1222/1222 [==============================] - 0s 299us/step - loss: 9.2544 - acc: 0.4255 - val_loss: 9.1563 - val_acc: 0.4281\n",
      "Epoch 2/5\n",
      "1222/1222 [==============================] - 0s 20us/step - loss: 8.6483 - acc: 0.4624 - val_loss: 8.6627 - val_acc: 0.4608\n",
      "Epoch 3/5\n",
      "1222/1222 [==============================] - 0s 22us/step - loss: 8.1620 - acc: 0.4918 - val_loss: 8.9018 - val_acc: 0.4477\n",
      "Epoch 4/5\n",
      "1222/1222 [==============================] - 0s 32us/step - loss: 7.8322 - acc: 0.5123 - val_loss: 7.1544 - val_acc: 0.5556\n",
      "Epoch 5/5\n",
      "1222/1222 [==============================] - 0s 33us/step - loss: 6.7825 - acc: 0.5786 - val_loss: 7.3639 - val_acc: 0.5425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174f64217b8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will...\n",
    "two_layer_model.fit(train_x,\n",
    "                    train_labels_categorical,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=5,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 44us/step\n",
      "Test accuracy: 0.5890052374744914\n"
     ]
    }
   ],
   "source": [
    "# This code will...\n",
    "test_loss, test_acc = two_layer_model.evaluate(test_x, test_labels_categorical)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "The confusion matrix is as follows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          110  109\n",
       "1           48  115"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will...\n",
    "#predicted_test_labels = shallow_single_layer_mod.predict(test_x)  # if the performance of the single layer model is better, use this line\n",
    "predicted_test_labels = two_layer_model.predict(test_x) # if the performance of the two layer model is better, use this line\n",
    "\n",
    "predict_y= []\n",
    "for row in predicted_test_labels:\n",
    "    if row[0] < row[1]:\n",
    "        predict_y.append(1)\n",
    "    else:\n",
    "        predict_y.append(0)\n",
    "    \n",
    "#predicted_test_y = np.where(np.array(predicted_test_labels) == [0,1], 1, 0)\n",
    "print(type(test_y))\n",
    "#test_set_error = np.mean(predicted_test_labels != test_y)\n",
    "#print(\"Test_set_error of Random Forest with the best b:\", test_set_error)\n",
    "# Create a confusion matrix#\n",
    "print(\"The confusion matrix is as follows:\")\n",
    "cm_nn = pd.crosstab(test_y, np.array(predict_y), rownames=['Actual'], colnames=['Predicted'])\n",
    "cm_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) 2-layer neural net without dimensional reduction\n",
    "We also tried neural network without dimensional reduction (we used data before conducting PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1528, 150300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will redefine training and test data  \n",
    "train_x_original = data[train_indices]\n",
    "train_y_original = y[train_indices]\n",
    "test_x_original = data[test_indices]\n",
    "test_y_original = y[test_indices]\n",
    "\n",
    "train_labels_categorical_original = to_categorical(train_y_original)\n",
    "test_labels_categorical_original = to_categorical(test_y_original)\n",
    "\n",
    "train_x_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will specify a model\n",
    "two_layer_model = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers\n",
    "two_layer_model.add(layers.Dense(250, activation='relu', input_shape=(150300,))) #optimized to get best accuracy at 250\n",
    "two_layer_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "two_layer_model.compile(optimizer='rmsprop',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1222 samples, validate on 306 samples\n",
      "Epoch 1/5\n",
      "1222/1222 [==============================] - 17s 14ms/step - loss: 6.2520 - acc: 0.6121 - val_loss: 6.2681 - val_acc: 0.6111\n",
      "Epoch 2/5\n",
      "1222/1222 [==============================] - 14s 12ms/step - loss: 6.2520 - acc: 0.6121 - val_loss: 6.2681 - val_acc: 0.6111\n",
      "Epoch 3/5\n",
      "1222/1222 [==============================] - 14s 12ms/step - loss: 6.2520 - acc: 0.6121 - val_loss: 6.2681 - val_acc: 0.6111\n",
      "Epoch 4/5\n",
      "1222/1222 [==============================] - 14s 12ms/step - loss: 6.2520 - acc: 0.6121 - val_loss: 6.2681 - val_acc: 0.6111\n",
      "Epoch 5/5\n",
      "1222/1222 [==============================] - 14s 11ms/step - loss: 6.2520 - acc: 0.6121 - val_loss: 6.2681 - val_acc: 0.6111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174d41c89e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will fit the neural net to the training set.\n",
    "two_layer_model.fit(train_x_original,\n",
    "                    train_labels_categorical_original,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=5,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 1s 4ms/step\n",
      "Test accuracy: 0.5732984290073055\n"
     ]
    }
   ],
   "source": [
    "# This code will compute test accuracy of the model.\n",
    "test_loss_original, test_acc_original = two_layer_model.evaluate(test_x_original, test_labels_categorical_original)\n",
    "print('Test accuracy:', test_acc_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the test accuracy of 2-layer neural net without dimensional reduction is 57.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, 2-layer neural network with dimensional reduction perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (3) Selecting the Best Classifier (Performance Evaluation)\n",
    "We evaluated the models with the best hyperparameters by comparing the expected cost (pay-off) of each model as follows.\n",
    "  \n",
    "First, we constructed a Cost Preference Matrix with points allotted to all outcomes (TP, TN, FP, FN) in a confusion matrix.  \n",
    "In the matrix, we penalized more where actual condition of building is \"damaged\" but classified as \"undamaged\" (FN). This is because government officials might overlook the intensity of damage.  \n",
    "  \n",
    "After constructing the matrix, we computed the expected costs (pay-off) of each classifier to select the best classifier, using a cost preference matrix and the confusion matrixes derived from each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  10  -5\n",
       "1 -10  10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will create a cost preference matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Creates a list containing 2 lists, each of 2 items, all set to 0\n",
    "w, h = 2, 2;\n",
    "cost_matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "cost_matrix[0][0] = 10\n",
    "cost_matrix[0][1] = 5*(-1) \n",
    "cost_matrix[1][0] = 10*(-1)\n",
    "cost_matrix[1][1] = 10\n",
    "\n",
    "df = pd.DataFrame(cost_matrix)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will define a function to sum up all the elements in a matrix.\n",
    "def get_sum_elements(arr):\n",
    "    sum_array = 0\n",
    "    for row in range(len(arr)):\n",
    "        for col in range(len(arr[row])):\n",
    "            sum_array = sum_array+ arr[row][col]\n",
    "        \n",
    "    return sum_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[530, 1675, 1445, 1225]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will calculate the expected cost (pay-off) for each model\n",
    "cost = []\n",
    "cost.append(get_sum_elements(cm_svc * df))\n",
    "cost.append(get_sum_elements(cm_knn * df))\n",
    "cost.append(get_sum_elements(cm_rf * df))\n",
    "cost.append(get_sum_elements(cm_nn * df))\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, (k-NN's pay-off is the highest)  \n",
    "    \n",
    "In conclusion, (k-NN is the best classifier)\n",
    "  \n",
    "The reason why k-Nearest Neighbor was the best might be...\n",
    "  \n",
    "Through industry experience we know that Neural Network is the state-of-art machine learning technique for accurate image analysis; however,... (challenges explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference List\n",
    "- yhat, \"Content-based image classification in Python\", June 12, 2013, http://blog.yhat.com/posts/image-classification-in-Python.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
